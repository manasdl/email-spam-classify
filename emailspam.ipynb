{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd840c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv('spam.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed751a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last 3 cols\n",
    "df.drop(columns=['Unnamed: 2','Unnamed: 3','Unnamed: 4'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2f554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the cols\n",
    "df.rename(columns={'v1':'target','v2':'text'},inplace=True)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c717b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "df['target'] = encoder.fit_transform(df['target'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d6ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing values\n",
    "df.isnull().sum()\n",
    "\n",
    "# check for duplicate values\n",
    "df.duplicated().sum()\n",
    "\n",
    "# remove duplicates\n",
    "df = df.drop_duplicates(keep='first')\n",
    "df.duplicated().sum()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c65a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89307c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.pie(df['target'].value_counts(), labels=['ham','spam'],autopct=\"%0.2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31693db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is imbalanced\n",
    "import nltk\n",
    "!pip install nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19befbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_characters'] = df['text'].apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# num of words\n",
    "df['num_words'] = df['text'].apply(lambda x:len(nltk.word_tokenize(x)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335ade86",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45eba955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ham\n",
    "df[df['target'] == 0][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194140b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spam\n",
    "df[df['target'] == 1][['num_characters','num_words','num_sentences']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab811005",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target'] == 0]['num_characters'])\n",
    "sns.histplot(df[df['target'] == 1]['num_characters'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fed463",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "sns.histplot(df[df['target'] == 0]['num_words'])\n",
    "sns.histplot(df[df['target'] == 1]['num_words'],color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d979660",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(df,hue='target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ce1b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr(),annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Data Preprocessing\n",
    "#Lower case\n",
    "#Tokenization\n",
    "#Removing special characters\n",
    "#Removing stop words and punctuation\n",
    "#Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08efc3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    text = text.lower()\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "    \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "            \n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "    \n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "            \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34522c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_text(\"I'm gonna be home soon and i don't want to talk about this stuff anymore tonight, k? I've cried enough today.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bd9594",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "ps.stem('loving')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df838b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['transformed_text'] = df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1da503",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1157d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "wc = WordCloud(width=500,height=500,min_font_size=10,background_color='white')\n",
    "spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab704b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(ham_wc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e156b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_corpus = []\n",
    "for msg in df[df['target'] == 1]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        spam_corpus.append(word)\n",
    "        \n",
    "len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab88f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sns.barplot(pd.DataFrame(Counter(spam_corpus).most_common(30))[0],pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4915c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_corpus = []\n",
    "for msg in df[df['target'] == 0]['transformed_text'].tolist():\n",
    "    for word in msg.split():\n",
    "        ham_corpus.append(word)\n",
    "len(ham_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f353185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "sns.barplot(pd.DataFrame(Counter(ham_corpus).most_common(30))[0],pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2126c43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Vectorization\n",
    "# using Bag of Words\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad49ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Model Building\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(max_features=3000)\n",
    "X = tfidf.fit_transform(df['transformed_text']).toarray()\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "#scaler = MinMaxScaler()\n",
    "#X = scaler.fit_transform(X)\n",
    "# appending the num_character col to X\n",
    "#X = np.hstack((X,df['num_characters'].values.reshape(-1,1)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767916aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['target'].values\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=2)\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix,precision_score\n",
    "gnb = GaussianNB()\n",
    "mnb = MultinomialNB()\n",
    "bnb = BernoulliNB()\n",
    "gnb.fit(X_train,y_train)\n",
    "y_pred1 = gnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred1))\n",
    "print(confusion_matrix(y_test,y_pred1))\n",
    "print(precision_score(y_test,y_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab139496",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.fit(X_train,y_train)\n",
    "y_pred2 = mnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred2))\n",
    "print(confusion_matrix(y_test,y_pred2))\n",
    "print(precision_score(y_test,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c99de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb.fit(X_train,y_train)\n",
    "y_pred3 = bnb.predict(X_test)\n",
    "print(accuracy_score(y_test,y_pred3))\n",
    "print(confusion_matrix(y_test,y_pred3))\n",
    "print(precision_score(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13261bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0)\n",
    "knc = KNeighborsClassifier()\n",
    "mnb = MultinomialNB()\n",
    "dtc = DecisionTreeClassifier(max_depth=5)\n",
    "lrc = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "rfc = RandomForestClassifier(n_estimators=50, random_state=2)\n",
    "abc = AdaBoostClassifier(n_estimators=50, random_state=2)\n",
    "bc = BaggingClassifier(n_estimators=50, random_state=2)\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)\n",
    "xgb = XGBClassifier(n_estimators=50,random_state=2)\n",
    "clfs = {\n",
    "    'SVC' : svc,\n",
    "    'KN' : knc, \n",
    "    'NB': mnb, \n",
    "    'DT': dtc, \n",
    "    'LR': lrc, \n",
    "    'RF': rfc, \n",
    "    'AdaBoost': abc, \n",
    "    'BgC': bc, \n",
    "    'ETC': etc,\n",
    "    'GBDT':gbdt,\n",
    "    'xgb':xgb\n",
    "}\n",
    "def train_classifier(clf,X_train,y_train,X_test,y_test):\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test,y_pred)\n",
    "    precision = precision_score(y_test,y_pred)\n",
    "    \n",
    "    return accuracy,precision\n",
    "train_classifier(svc,X_train,y_train,X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16490d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_scores = []\n",
    "precision_scores = []\n",
    "\n",
    "for name,clf in clfs.items():\n",
    "    \n",
    "    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)\n",
    "    \n",
    "    print(\"For \",name)\n",
    "    print(\"Accuracy - \",current_accuracy)\n",
    "    print(\"Precision - \",current_precision)\n",
    "    \n",
    "    accuracy_scores.append(current_accuracy)\n",
    "    precision_scores.append(current_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b2cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy':accuracy_scores,'Precision':precision_scores}).sort_values('Precision',ascending=False)\n",
    "performance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70a7570",
   "metadata": {},
   "outputs": [],
   "source": [
    "performance_df1 = pd.melt(performance_df, id_vars = \"Algorithm\")\n",
    "performance_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.catplot(x = 'Algorithm', y='value', \n",
    "               hue = 'variable',data=performance_df1, kind='bar',height=5)\n",
    "plt.ylim(0.5,1.0)\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5236dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model improve\n",
    "# 1. Change the max_features parameter of TfIdf\n",
    "temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_max_ft_3000':accuracy_scores,'Precision_max_ft_3000':precision_scores}).sort_values('Precision_max_ft_3000',ascending=False)\n",
    "temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_scaling':accuracy_scores,'Precision_scaling':precision_scores}).sort_values('Precision_scaling',ascending=False)\n",
    "new_df = performance_df.merge(temp_df,on='Algorithm')\n",
    "new_df_scaled = new_df.merge(temp_df,on='Algorithm')\n",
    "temp_df = pd.DataFrame({'Algorithm':clfs.keys(),'Accuracy_num_chars':accuracy_scores,'Precision_num_chars':precision_scores}).sort_values('Precision_num_chars',ascending=False)\n",
    "new_df_scaled.merge(temp_df,on='Algorithm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bda982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voting Classifier\n",
    "svc = SVC(kernel='sigmoid', gamma=1.0,probability=True)\n",
    "mnb = MultinomialNB()\n",
    "etc = ExtraTreesClassifier(n_estimators=50, random_state=2)\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "voting = VotingClassifier(estimators=[('svm', svc), ('nb', mnb), ('et', etc)],voting='soft')\n",
    "voting.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33a1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = voting.predict(X_test)\n",
    "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision\",precision_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa87b675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying stacking\n",
    "estimators=[('svm', svc), ('nb', mnb), ('et', etc)]\n",
    "final_estimator=RandomForestClassifier()\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "clf = StackingClassifier(estimators=estimators, final_estimator=final_estimator)\n",
    "clf.fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(\"Accuracy\",accuracy_score(y_test,y_pred))\n",
    "print(\"Precision\",precision_score(y_test,y_pred))\n",
    "\n",
    "import pickle\n",
    "pickle.dump(tfidf,open('vectorizer.pkl','wb'))\n",
    "pickle.dump(mnb,open('model.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
